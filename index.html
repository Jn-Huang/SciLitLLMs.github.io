<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="https://scilitllms.github.io"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding?</title>
  <link rel="icon" type="image/png" href="static/images/Iconoir-Team-Iconoir-Microscope.512.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding?</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=1uFZs6EAAAAJ&hl=en">Sihang Li</a>*<sup style="color:#005587;">1</sup>,
              </span>
              <span class="author-block">
                <a href="https://jn-huang.github.io">Jin Huang</a>*<sup style="color:#FFD700;">3</sup>,
              </span>
              <span class="author-block">
                <a href="">Jiaxi Zhuang</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=EWU3rdIAAAAJ&hl=en">Yaorui Shi</a><sup style="color:#005587;">1</sup>,
              </span>
              <span class="author-block">
                <a href="">Xiaochen Cai</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="">Mingjun Xu</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="https://xiangwang1223.github.io">Xiang Wang</a>‡<sup style="color:#005587;">1</sup>,
              </span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=jk7qwmcAAAAJ&hl=zh-CN">Linfeng Zhang</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="https://guolinke.github.io">Guolin Ke</a><sup style="color:#8C1515;">2</sup>,
              </span>
              <span class="author-block">
                <a href="">Hengxing Cai</a>‡<sup style="color:#8C1515;">2</sup>
              </span>
              <br>

              <div class="is-size-5 publication-authors">
                <span class="author-block"><sup style="color:#005587;">1</sup>University of Science and Technology of China,</span>
                <span class="author-block"><sup style="color:#8C1515">2</sup>DP Technology,</span>
                <span class="author-block"><sup style="color:#FFD700">3</sup>University of Michigan</span><br>
              
                <span class="paper-block"><b>* Equal Contribution, ‡ Corresponding author</b></span><br>
                <!-- <span class="paper-block"><b>† Work done while interning at DP Technology</b></span><br> -->
                <span class="paper-block"><b></b></span><br>
                <span class="paper-block"><b>In submission. To appear at Foundation Models for Science Workshop, NeurIPS 2024</b></span>
              </div>
              
              <!-- <span class="author-block">
                <a href="FIRST AUTHOR PERSONAL LINK" target="_blank">First Author</a><sup>*</sup>,</span>
                <span class="author-block">
                  <a href="SECOND AUTHOR PERSONAL LINK" target="_blank">Second Author</a><sup>*</sup>,</span>
                  <span class="author-block">
                    <a href="THIRD AUTHOR PERSONAL LINK" target="_blank">Third Author</a>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block">Institution Name<br>Conferance name and year</span>
                    <span class="eql-cntrb"><small><br><sup>*</sup>Indicates Equal Contribution</small></span>
                  </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://arxiv.org/abs/2408.15545" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="ai ai-arxiv"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <!-- <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/dptech-corp/Uni-SMART/tree/main/SciLitLLM" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- LLMs -->
                <span class="link-block">
                  <a href="https://huggingface.co/Uni-SMART/SciLitLLM-1.5" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <!-- <i class="fas fa-hugging-face"></i> -->
                    <i class="fa-solid fa-database"></i>
                  </span>
                  <span>Models</span>
                </a>
                
                <!-- Datasets -->
                <span class="link-block">
                  <a href="https://huggingface.co/datasets/Uni-SMART/SciLitIns" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <!-- <i class="fas fa-hugging-face"></i> -->
                    <i class="fa-solid fa-database"></i>
                  </span>
                  <span>Datasets</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>






<section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="static/images/performance_plot.png" alt="Performance comparison" width="80%"/>
      <p> 
        Performance of <b>SciLitLLM</b> on scientific literature understanding benchmarks, comparing model sizes and parameters. SciLitLLMs outperform Llama3.1 and Qwen2.5 models with similar scales.
      </p>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Understanding scientific literature is crucial for extracting information that drives discovery. Despite the success of general-purpose large language models (LLMs), they struggle with scientific texts due to their unfamiliarity with domain-specific tasks and knowledge. To bridge this gap, we introduce <b>SciLitLLM</b>, a framework that combines continual pre-training (CPT) and supervised fine-tuning (SFT) to enhance both the domain knowledge and instruction-following capabilities of LLMs for scientific literature understanding.
          </p>
          <img src="figures/intro.png" alt="Framework" class="interpolation-image"/>
          <p>
            Our contributions include a new scientific instruction dataset, <b>SciLitIns</b>, and a hybrid strategy for adapting LLMs to specialized tasks. The proposed SciLitLLM demonstrates superior performance, achieving significant improvements, including a 10.1% performance boost on SciRIFF benchmarks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Scientific Knowledge Injection</h2>
        <div class="content has-text-justified">
          <p>
            To inject domain knowledge, we continually pre-train on a high-quality scientific corpus. This step ensures that the model can comprehend complex scientific concepts and terminologies. Our CPT stage utilized over 73,000 textbooks and 625,000 research papers, providing a strong foundation for the model’s understanding of scientific texts.
          </p>
          <img src="figures/knowledge_injection.png" alt="Knowledge Injection" width="80%"/>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Introduction</h2>
        <div class="content has-text-justified">
          <p>
            Understanding scientific literature is crucial for extracting information that drives discovery. Despite the success of general-purpose large language models (LLMs), they struggle with scientific texts due to their unfamiliarity with domain-specific tasks and knowledge. To bridge this gap, we introduce <b>SciLitLLM</b>, a framework that combines continual pre-training (CPT) and supervised fine-tuning (SFT) to enhance both the domain knowledge and instruction-following capabilities of LLMs for scientific literature understanding.
          </p>
          <img src="static/images/sci_lit_analysis_v3.png" alt="Problem Setup" class="interpolation-image"/>
          <p>
            Our contributions include a new scientific instruction dataset, <b>SciLitIns</b>, and a hybrid strategy for adapting LLMs to specialized tasks. The proposed SciLitLLM demonstrates superior performance, achieving significant improvements, including a 10.1% performance boost on SciRIFF benchmarks.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Scientific Knowledge Injection</h2>
        <div class="content has-text-justified">
          <p>
            To inject domain knowledge, we continually pre-train on a high-quality scientific corpus. This step ensures that the model can comprehend complex scientific concepts and terminologies. Our CPT stage utilized over 73,000 textbooks and 625,000 research papers, providing a strong foundation for the model’s understanding of scientific texts.
          </p>
          <img src="static/images/SciLLM-Framework_v5.png" alt="SciLitLLM Framework" width="80%"/>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
  <div class="container is-max-desktop">
    <div class="content has-text-centered">
      <img src="static/images/evaluation.png" alt="Performance comparison" width="80%"/>
      <p> 
        Performance of <b>SciLitLLM</b> on scientific literature understanding benchmarks, comparing model sizes and parameters. The framework outperforms Llama3.1 and Qwen2.5 models with similar scales, particularly excelling in diverse scientific tasks such as entity extraction and molecule generation.
      </p>
    </div>
  </div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{li2024scilitllm,
  title={SciLitLLM: How to Adapt LLMs for Scientific Literature Understanding},
  author={Li, Sihang and Huang, Jian and Zhuang, Jiaxi and Shi, Yaorui and Cai, Xiaochen and Xu, Mingjun and Wang, Xiang and Zhang, Linfeng and Ke, Guolin and Cai, Hengxing},
  journal={arXiv preprint arXiv:2408.15545},
  year={2024}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
